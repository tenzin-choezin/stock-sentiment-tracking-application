{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "73630869-43c4-48de-b1ba-fb222929a6c4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "334992a5",
    "execution_start": 1650843463969,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "#imports \nimport pandas as pd\nimport numpy as np\nimport requests \nfrom time import sleep\nfrom collections import Counter",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Todo \n* run model \n* get proper output ",
   "metadata": {
    "cell_id": "f0ee01c27fed4ec9a63186b152c00c5c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "10e20c68",
    "execution_start": 1650438906988,
    "execution_millis": 11,
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 159.78125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "41354cbd7b004baf8cbd5fd10325a954",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "31570130",
    "execution_start": 1650843464015,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1017
   },
   "source": "#from twitter\n\nheaders = {\"Authorization\": \"Bearer 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\"}\n\n# Model 1: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n\nmodel1 = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n\n# Model 2: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis \n\n# label dict not needed, output displays score + sentiment \nmodel2 = \"https://api-inference.huggingface.co/models/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n\n# Model 3: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\nmodel3 = \"https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english\"\n\n\ndef get_sentiment(string, model, type = None):\n    #string - text to run through model \n    #model - model url (reference above) \n    #output types: score, label \n    done = False\n    \n    headers = {\"Authorization\": \"Bearer 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\"}\n    while not done:\n        try: \n            #access model + obtain ouput\n            payload = query = {\"inputs\": string}\n            print(payload)\n            response = requests.post(model, headers = headers, json = query) \n            print(response.json())\n            output = response.json()[0]\n            print(output)\n\n            best = max(output, key = lambda x: x['score'])\n            label = best['label'].lower()\n            score = np.round(best['score'], decimals = 3)\n            done = True \n        except Exception as KeyError: \n            pass\n            if KeyError:\n                sleep(20)  \n    \n    #desired output\n    if type == \"score\": \n        return score\n    if type == \"label\": \n        return label\n\n    return label, score\n    \n\nvec_sentiment = np.vectorize(get_sentiment)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4906350f17be4eada0f656426cfb18f2",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2591f90d",
    "execution_start": 1650843464016,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 531
   },
   "source": "#gets sample post and comments \ndef get_text(post_titles, comments): \n    \n    if (len(post_titles) + len(comments)) < 150: \n        print('1')\n        sample_titles, sample_comments = post_titles, comments\n        text = np.append(sample_titles, sample_comments)\n        return text \n\n    if (len(post_titles) <= 30):    \n        sample_titles = post_titles\n\n    if len(post_titles) > 30: \n        sample_titles = np.random.choice(post_titles, 30)\n\n    size = 150 - len(sample_titles)\n\n    if size >= len(comments):\n        sample_comments = comments \n\n    elif size < len(comments): \n        sample_comments = np.random.choice(comments, size) \n\n    text = np.append(sample_titles, sample_comments)\n\n    return text",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a5c7ef1dd24540658d7ab06245a90a3c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3d5378ff",
    "execution_start": 1650843466392,
    "execution_millis": 2,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 963
   },
   "source": "#returns sentiment proportions + determines majority sentinment based \ndef process_Sentiment(sentimment_Data): \n    neg_m1 = sentimment_Data[sentimment_Data['model1Sentiment'] == 'negative']['model1Score'].values \n    neg_m2 = sentimment_Data[sentimment_Data['model2Sentiment'] == 'negative']['model2Score'].values\n    neg_m3 = sentimment_Data[sentimment_Data['model3Sentiment'] == 'negative']['model3Score'].values\n    neg_arr = np.concatenate((neg_m1, neg_m2, neg_m3), axis = None)\n    neg_avg = np.average(neg_arr)\n\n    pos_m1 = sentimment_Data[sentimment_Data['model1Sentiment'] == 'positive']['model1Score'].values \n    pos_m2 = sentimment_Data[sentimment_Data['model2Sentiment'] == 'positive']['model2Score'].values\n    pos_m3 = sentimment_Data[sentimment_Data['model3Sentiment'] == 'positive']['model3Score'].values\n    pos_arr = np.concatenate((pos_m1, pos_m2, pos_m3), axis = None)\n    pos_avg = np.average(pos_arr)\n\n    neu_m1 = sentimment_Data[sentimment_Data['model1Sentiment'] == 'neutral']['model1Score'].values\n    neu_m2 = sentimment_Data[sentimment_Data['model2Sentiment'] == 'neutral']['model2Score'].values  \n    neu_m3 = sentimment_Data[sentimment_Data['model3Sentiment'] == 'neutral']['model3Score'].values  \n    neu_arr = np.concatenate((neu_m1, neu_m2, neu_m3), axis = None)\n\n    total_vals = len(sentimment_Data) * 3 \n\n    positive_per = np.round((len(pos_arr)/total_vals) * 100, 2)\n    negative_per = np.round((len(neg_arr)/total_vals) * 100, 2)\n    neutral_per =  np.round((len(neu_arr)/total_vals) * 100, 2)\n\n    num_neg, num_pos = len(neg_arr), len(pos_arr)\n    neg_weight, pos_weight = num_neg / total_vals, num_pos / total_vals\n    weighted_neg, weighted_pos = neg_avg * neg_weight, pos_avg * pos_weight \n\n    try:\n        if (num_neg / num_pos) > 0.75 and (num_neg / num_pos) < 1.25:\n            sentimment_ratio = neg_avg / pos_avg\n        else: \n            sentimment_ratio = weighted_neg / weighted_pos\n            \n    except ZeroDivisionError:\n        sentimment_ratio = 2\n    \n    final_sentimment = 'Negative'\n    if sentimment_ratio == 1:\n        final_sentimment = np.random.choice('Negative', 'Positive')\n    elif sentimment_ratio < 1:\n        final_sentimment = 'Positive'\n    \n    output_dict = {'Negative Percent' : negative_per,\n                   'Positive Percent' : positive_per,\n                   'Neutral Percent': neutral_per,\n                   'Overall Sentiment' : final_sentimment}\n\n    return output_dict",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5a6d119f94d743c1a179d37538ea9936",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6d28dd9e",
    "execution_start": 1650843468339,
    "execution_millis": 3,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 279
   },
   "source": "#returns dictionary of dataframes for each stock ticker\ndef split_by_ticker(data): \n    stock_tickers = data['stock'].unique() \n\n    DataFrameDict = {elem : pd.DataFrame for elem in stock_tickers}\n    \n    for key in DataFrameDict.keys():\n        #process each dataframe \n        columns = {'full_text_preprocessed': 'text'}\n        DataFrameDict[key] = data[data['stock'] == key].copy().rename(columns = columns)\n    \n    return DataFrameDict",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a00e4b0a2fad4eb0b6843cf52b3b4d92",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fbf45d6f",
    "execution_start": 1650843469655,
    "execution_millis": 2,
    "owner_user_id": "c6e5cdc1-bb74-451b-b7c7-5f8661c67a10",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 531
   },
   "source": "def run_model(ticker_df): \n\n    models = [model1, model2, model3] \n    model_dict = {model1: 'model1', model2: 'model2', model3: 'model3'}\n    \n    post_titles = ticker_df[ticker_df['type'] == 'POST']['post_title'].unique()\n    long_comments = ticker_df[(ticker_df['type'] == 'COMMENT') & (ticker_df['selftext'] != '[deleted]')].dropna(subset = ['selftext'])\n    comments = long_comments[long_comments['selftext'].str.len() < 950]['selftext'].values\n    \n\n    mentions = (len(post_titles) + len(long_comments))\n    text = get_text(post_titles, comments)\n\n    data = pd.DataFrame({'text': text}).reset_index()\n\n\n    for model in models: \n        label = model_dict[model] + \"Sentiment\" \n        result_label = model_dict[model] + \"Score\"  \n\n        sentiment, score = vec_sentiment(text, model)\n        data[label] = sentiment \n        data[result_label] = score \n\n    return mentions, data\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4340484c689345de88644d52659bd785",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "46422323",
    "execution_start": 1650843618990,
    "execution_millis": 1,
    "owner_user_id": "db0bc935-8406-40e6-963a-acd0d64d7de9",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 621
   },
   "source": "def process_Reddit(data): \n    #get data dic\n    dataDic = split_by_ticker(data)\n    # get tickers \n\n    tickers = list(dataDic.keys())\n    mentions, negative_per, positive_per, neutral_per, overall = [], [], [], [], []\n\n    for ticker in tickers: \n        mention_count, stock = run_model(dataDic[ticker]) \n        results = process_Sentiment(stock)\n        \n        mentions.append(mention_count)\n        negative_per.append(results['Negative Percent'])\n        positive_per.append(results['Positive Percent'])\n        neutral_per.append(results['Neutral Percent'])\n        overall.append(results['Overall Sentiment']) \n\n\n    \n    output = pd.DataFrame({\"Ticker\": tickers,\n                          \"Mentions\": mentions, \n                          \"Negative Percent\": negative_per, \n                          \"Positive Percent\": positive_per, \n                          \"Neutral Percent\": neutral_per, \n                          \"Overall Sentiment\": overall\n                            })\n    \n    return output\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "233d4353fcb340d08135940cefcb3cae",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 66
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=643f0a0a-e649-4860-b73b-f3561d8b41c9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "b0d1bf96-dd27-424f-aca3-baa29a8b6bda",
  "deepnote_execution_queue": []
 }
}