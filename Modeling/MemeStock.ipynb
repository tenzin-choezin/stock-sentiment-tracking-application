{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LoZ28tIL7KV"
   },
   "source": [
    "# Twitter API\n",
    "## By: Chris Trojniak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08uM5lSIM0fk",
    "outputId": "2b42392f-c4fb-4549-ca26-455bc8b3ad2d"
   },
   "outputs": [],
   "source": [
    "#!pip install preprocessor\n",
    "#!pip install tweet-preprocessor\n",
    "#!pip install git+https://github.com/tweepy/tweepy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UduoNWOSMRJ2"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import preprocessor as pre\n",
    "import regex as re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8m44zeYSL-8i"
   },
   "outputs": [],
   "source": [
    "def pull_twitter(): \n",
    "\n",
    "    # Set tickers and max tweets\n",
    "    tickers = ['GME','TSLA','TWTR','AMC','SPY','HMHC','DWAC','AMD','SST','AAPL','AMZN','NVDA','TLRY','NFLX','QQQ','PLTR','FB','BABA','VIX','SOFI','TEAM','RBLX','RSX','WISH','OSU']\n",
    "    max_tweets = 5000 #max per ticker \n",
    "    #tickers = ['TSLA']\n",
    "\n",
    "    # Set date/time parameters\n",
    "    cur_time_utc = datetime.utcnow().replace(microsecond=0)\n",
    "    until_date = cur_time_utc.strftime(\"%Y-%m-%d\") #\"2022-04-03\"#\"2022-04-25\"#\n",
    "    from_date =  cur_time_utc - timedelta(days=1) #\"2022-04-02\"#\"2022-04-24\"#\n",
    "    from_date = from_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Authentication\n",
    "    consumer_key = \"juf1eoH4Gn5ZqCtsuKm7PUdAO\"\n",
    "    consumer_secret = \"7VX0jDqBKTAjVzvYiKhvW4b2tAebMc38FzmPrpcnILkJRt5tF5\"\n",
    "    auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "    # Create a wrapper for the Twitter API\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    # Function: preprocess tweet text\n",
    "    def tweetprocess(tweet):\n",
    "        #https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "        cleantweet = pre.clean(tweet)\n",
    "        cleantweet = cleantweet.lower()\n",
    "        cleantweet = re.sub(\"\\d+\", \"\", cleantweet)\n",
    "        cleantweet = re.sub(r'[^\\w\\s]', '', cleantweet)   \n",
    "        return cleantweet\n",
    "\n",
    "    # Function: identify tweet quality\n",
    "    def tweetquality(user_verified, favorite_count, retweet_count):\n",
    "        if user_verified == True or favorite_count > 100 or retweet_count > 10:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Function: twitter search pagination and rate limit handling\n",
    "    def limit_handled(cursor):\n",
    "      while True:\n",
    "          try:\n",
    "              yield cursor.next()\n",
    "          except StopIteration:\n",
    "              break\n",
    "          except (tweepy.errors, Exception):\n",
    "              print('Reached rate limit. Sleeping for >15 minutes')\n",
    "              time.sleep(15 * 61)\n",
    "          except Exception as e: \n",
    "              print(e)\n",
    "              pass\n",
    "\n",
    "    # Function: obtain query list from ticker list\n",
    "    def getqueries(tickers): #returns a list of query strings\n",
    "        queries = []\n",
    "        for ticker in tickers:\n",
    "            querylist = []\n",
    "            querylist.append('$'+ticker)\n",
    "            othertickers=[]\n",
    "            [othertickers.append(t) for t in tickers]\n",
    "            othertickers.remove(ticker)\n",
    "            for otherticker in othertickers:\n",
    "                querylist.append(' -$'+otherticker) \n",
    "            tickerquery = ''.join([str(q) for q in querylist]) + ' -filter:retweets' #Exclude retweets\n",
    "            queries.append(tickerquery)\n",
    "        return queries\n",
    "\n",
    "    # Function: get tweets using tweepy\n",
    "    def get_tweets(query, since_id, until_date, max_tweets):\n",
    "\n",
    "        #search for tweets using Tweepy\n",
    "        search = limit_handled(tweepy.Cursor(api.search_tweets #https://docs.tweepy.org/en/stable/api.html#search-tweets\n",
    "                            ,q=query\n",
    "                            ,tweet_mode='extended'\n",
    "                            ,lang='en'\n",
    "                            ,result_type=\"recent\"\n",
    "                            ,since_id = since_id\n",
    "                            ,until=until_date\n",
    "                            ).items(max_tweets))\n",
    "        dftweets = pd.DataFrame()\n",
    "        for tweet in search:\n",
    "            dftweets = dftweets.append(pd.json_normalize(tweet._json))\n",
    "        print(query,'\\n','# tweets:',len(dftweets))\n",
    "\n",
    "        # calculate attributes\n",
    "        try:\n",
    "            dftweets['full_text_preprocessed'] = dftweets.apply(lambda row : tweetprocess(row['full_text']), axis = 1)\n",
    "            dftweets['quality'] = dftweets.apply(lambda row : tweetquality(row['user.verified'], row['favorite_count'], row['retweet_count']), axis = 1)\n",
    "            dftweets['num_cashtags'] = dftweets.apply(lambda row : str(row['entities.symbols']).count('text'), axis = 1)\n",
    "            dftweets['ticker'] = dftweets.apply(lambda row : query.split()[0], axis = 1)\n",
    "            dftweets['query_params'] = dftweets.apply(lambda row : 'query:'+query+' since_id:'+str(since_id)+' until_date:'+str(until_date)+' max_tweets:'+str(max_tweets), axis = 1)\n",
    "            #apply filter\n",
    "            dftweets = dftweets[dftweets.num_cashtags == 1]\n",
    "            # return output\n",
    "            return dftweets[['id','ticker','created_at','full_text_preprocessed','user.verified','favorite_count','retweet_count','quality','entities.symbols','num_cashtags','query_params']]\n",
    "        except Exception:\n",
    "            pass\n",
    "        finally:\n",
    "            print(' # tweets (filtered):',len(dftweets),'\\n')\n",
    "\n",
    "      \n",
    "\n",
    "    # Find the last tweet id for from_date (need this to filter on from_date)\n",
    "    search_since_id = limit_handled(tweepy.Cursor(api.search_tweets #https://docs.tweepy.org/en/stable/api.html#search-tweets\n",
    "                            ,q='A'\n",
    "                            ,tweet_mode='extended'\n",
    "                            ,lang='en'\n",
    "                            ,result_type=\"recent\"\n",
    "                            ,until=from_date\n",
    "                            ).items(1))\n",
    "    since_id  = [tweet._json['id'] for tweet in search_since_id][0]\n",
    "\n",
    "    #Get the tweets\n",
    "    queries = getqueries(tickers)\n",
    "    dftweets_tofile = pd.DataFrame()\n",
    "    for query in queries:\n",
    "        dftweets_tofile = dftweets_tofile.append(get_tweets(query, since_id, until_date, max_tweets))\n",
    "\n",
    "    return dftweets_tofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8QiCZfSOJgS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HdEuN-hNxA_"
   },
   "source": [
    "# Reddit API \n",
    "## By Jennifer Hu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuuOJBaHQVfq",
    "outputId": "57794439-eb64-4268-d395-7e99fe9b5b8d"
   },
   "outputs": [],
   "source": [
    "#!pip install praw\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1si_EqUkOPYs"
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"nRTaqFD5JTYnnY7WbXpFyg\",\n",
    "    client_secret=\"3jq2QPbD6WwOO9r1BP5yAvOoJ1tFCg\",\n",
    "    password=\"$NNK46p.?!@fqRg\",\n",
    "    user_agent=\"FinanceBot/0.0.1\",\n",
    "    username=\"IdleIn\",\n",
    "    check_for_async=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZYpKyFbCOv4H"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from praw.models import MoreComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "60iJaiZWOv8g"
   },
   "outputs": [],
   "source": [
    "#returns df of reddit posts for model\n",
    "def pull_reddit(): \n",
    "  #stocklist = ('gme', 'amc', 'tsla', 'hood', 'fb', 'twtr', 'spy', 'webr', 'evtl', 'arqq', 'bbby', 'tsm', 'ppi', 'nvda')\n",
    "  stocklist = ('nflx','tsla', 'gme','twtr','spy', 'fb','amc', 'dis','nvda', 'amd', 'amzn', 'snap', 'vix', 'qqq', 'aapl', 'tlry', 'hmhc')\n",
    "  subreddits = ('wallstreetbets','StockMarket','pennystocks','GME','CryptoCurrency','stocks','investing','Superstonk')\n",
    "  \n",
    "  posts = pd.DataFrame()\n",
    "  for stock in stocklist:\n",
    "    for sub in subreddits:\n",
    "      subreddit = reddit.subreddit(sub)\n",
    "      for post in subreddit.search(stock, sort = 'new', time_filter = \"week\",limit=None):\n",
    "        if stock not in post.title.lower():\n",
    "          continue\n",
    "        pslice = pd.DataFrame()\n",
    "        pslice = pslice.append({\n",
    "            'stock': stock,\n",
    "            'subreddit': post.subreddit,\n",
    "            'type': 'POST',\n",
    "            'post_title': post.title,\n",
    "            'selftext': post.selftext,\n",
    "            #'upvote_ratio': post.upvote_ratio,\n",
    "            #'ups': post.ups,\n",
    "            'ups': post.score,\n",
    "            #'num_comments': post.num_comments,\n",
    "            'id': post.id,\n",
    "            'time': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "          }, ignore_index=True)\n",
    "        #if post.id in posts['id']:\n",
    "          #continue\n",
    "        posts = posts.append(pslice, ignore_index=True)\n",
    "        #post.comments.replace_more(limit=None)\n",
    "        cslice = pd.DataFrame()\n",
    "        for com in post.comments:\n",
    "          if isinstance(com, MoreComments):\n",
    "            continue\n",
    "          cslice = cslice.append({\n",
    "              'stock': stock,\n",
    "              'subreddit': post.subreddit,\n",
    "              'type': 'COMMENT',\n",
    "              'post_title': post.title,\n",
    "              'selftext': com.body,\n",
    "              #'upvote_ratio': com.upvote_ratio,\n",
    "              #'ups': com.ups,\n",
    "              'ups': com.score,\n",
    "              #'num_comments': post.num_comments,\n",
    "              'id': post.id,\n",
    "              'time': datetime.fromtimestamp(com.created_utc).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            }, ignore_index=True)\n",
    "        posts = posts.append(cslice, ignore_index=True)\n",
    "\n",
    "  return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RGz5dGTOwbm"
   },
   "source": [
    "# Model \n",
    "## By Dennis Le & Tenzin Choezin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Kf17AQFmO66Z"
   },
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "from time import sleep\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sMMa9Jq5O0Lq"
   },
   "outputs": [],
   "source": [
    "#sentiment function both reddit and twitter \n",
    "\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "# Model 1: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "\n",
    "model1 = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "# Model 2: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis \n",
    "\n",
    "# label dict not needed, output displays score + sentiment \n",
    "model2 = \"https://api-inference.huggingface.co/models/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "\n",
    "# Model 3: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
    "model3 = \"https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "def get_sentiment(string, model, type = None):\n",
    "    #string - text to run through model \n",
    "    #model - model url (reference above) \n",
    "    #output types: score, label \n",
    "    done = False\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "    while not done:\n",
    "        try: \n",
    "            #access model + obtain ouput\n",
    "            payload = query = {\"inputs\": string}\n",
    "            #print(payload)\n",
    "            response = requests.post(model, headers = headers, json = query) \n",
    "            #print(response.json())\n",
    "            output = response.json()[0]\n",
    "            #print(output)\n",
    "\n",
    "            best = max(output, key = lambda x: x['score'])\n",
    "            label = best['label'].lower()\n",
    "            score = np.round(best['score'], decimals = 3)\n",
    "            done = True \n",
    "        except Exception as KeyError: \n",
    "            pass\n",
    "            if KeyError:\n",
    "                sleep(20)  \n",
    "    \n",
    "    #desired output\n",
    "    if type == \"score\": \n",
    "        return score\n",
    "    if type == \"label\": \n",
    "        return label\n",
    "\n",
    "    return label, score\n",
    "    \n",
    "\n",
    "vec_sentiment = np.vectorize(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yuEYsDQUPkxF"
   },
   "outputs": [],
   "source": [
    "#reddit\n",
    "#gets sample post and comments \n",
    "def get_text(post_titles, comments): \n",
    "    \n",
    "    if (len(post_titles) + len(comments)) < 150: \n",
    "        sample_titles, sample_comments = post_titles, comments\n",
    "        text = np.append(sample_titles, sample_comments)\n",
    "        return text \n",
    "\n",
    "    if (len(post_titles) <= 30):    \n",
    "        sample_titles = post_titles\n",
    "\n",
    "    if len(post_titles) > 30: \n",
    "        sample_titles = np.random.choice(post_titles, 30)\n",
    "\n",
    "    size = 150 - len(sample_titles)\n",
    "\n",
    "    if size >= len(comments):\n",
    "        sample_comments = comments \n",
    "\n",
    "    elif size < len(comments): \n",
    "        sample_comments = np.random.choice(comments, size) \n",
    "\n",
    "    text = np.append(sample_titles, sample_comments)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SXtXUOxgO3EE"
   },
   "outputs": [],
   "source": [
    "#returns dictionary of dataframes for each stock ticker\n",
    "\n",
    "#returns dictionary of dataframes for each stock ticker\n",
    "def split_by_ticker(data, column):\n",
    "    if column == \"Reddit\": \n",
    "       text = 'stock'\n",
    "    if column == \"Twitter\": \n",
    "       text = 'ticker' \n",
    "\n",
    "    stock_tickers = data[text].unique() \n",
    "    DataFrameDict = {elem : pd.DataFrame for elem in stock_tickers}\n",
    "  \n",
    "    for key in DataFrameDict.keys():\n",
    "        if column == \"Twitter\":\n",
    "          columns = {'full_text_preprocessed': 'text'}\n",
    "          DataFrameDict[key] = data[data[text] == key].copy().rename(columns = columns)\n",
    "        else: \n",
    "          DataFrameDict[key] = data[data[text] == key].copy()\n",
    "\n",
    "    return DataFrameDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6PJueWa7O_bk"
   },
   "outputs": [],
   "source": [
    "def run_Reddit(ticker_df): \n",
    "\n",
    "    models = [model1, model2, model3] \n",
    "    model_dict = {model1: 'model1', model2: 'model2', model3: 'model3'}\n",
    "    \n",
    "    post_titles = ticker_df[ticker_df['type'] == 'POST']['post_title'].unique()\n",
    "    long_comments = ticker_df[(ticker_df['type'] == 'COMMENT') & (ticker_df['selftext'] != '[deleted]')].dropna(subset = ['selftext'])\n",
    "    comments = long_comments[long_comments['selftext'].str.len() < 1000]['selftext'].values\n",
    "    \n",
    "\n",
    "    mentions = (len(post_titles) + len(long_comments))\n",
    "    text = get_text(post_titles, comments)\n",
    "\n",
    "    data = pd.DataFrame({'text': text}).reset_index()\n",
    "\n",
    "\n",
    "    for model in models: \n",
    "        label = model_dict[model] + \"Sentiment\" \n",
    "        result_label = model_dict[model] + \"Score\"  \n",
    "\n",
    "        sentiment, score = vec_sentiment(text, model)\n",
    "        data[label] = sentiment \n",
    "        data[result_label] = score \n",
    "\n",
    "    return mentions, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LFTGB3NOPBTk"
   },
   "outputs": [],
   "source": [
    "#run sentiment model on dataframe for one stock ticker\n",
    "#returns mentions and tuple -  (sentiment, score)\n",
    "def run_Twitter(ticker_df):  \n",
    "    \n",
    "    models = [model1, model2, model3] \n",
    "    model_dict = {model1: 'model1', model2: 'model2', model3: 'model3'}\n",
    "    mentions = len(ticker_df) \n",
    "\n",
    "    ticker_df_filt = ticker_df[(ticker_df['favorite_count'] >= 2) | (ticker_df['retweet_count'] >= 1)].copy()\n",
    "    \n",
    "    if len(ticker_df_filt) > 150:\n",
    "        data = ticker_df_filt.sample(150)\n",
    "\n",
    "    elif len(ticker_df_filt) < 30: \n",
    "        if len(ticker_df) > 150:\n",
    "            data = ticker_df.sample(150) \n",
    "        else: \n",
    "            data = ticker_df\n",
    "\n",
    "    else:\n",
    "        data = ticker_df_filt.copy()\n",
    "\n",
    "    text = data['text'].values \n",
    "    \n",
    "    for model in models: \n",
    "        label = model_dict[model] + \"Sentiment\" \n",
    "        result_label = model_dict[model] + \"Score\"  \n",
    "\n",
    "        sentiment, score = vec_sentiment(text, model)\n",
    "        data[label] = sentiment \n",
    "        data[result_label] = score \n",
    "\n",
    "    return mentions, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HvjCvir6PClT"
   },
   "outputs": [],
   "source": [
    "#returns sentiment proportions + determines majority sentinment based \n",
    "def process_Sentiment(sentimment_Data): \n",
    "    neg_m1 = sentimment_Data[sentimment_Data['model1Sentiment'] == 'negative']['model1Score'].values \n",
    "    neg_m2 = sentimment_Data[sentimment_Data['model2Sentiment'] == 'negative']['model2Score'].values\n",
    "    neg_m3 = sentimment_Data[sentimment_Data['model3Sentiment'] == 'negative']['model3Score'].values\n",
    "    neg_arr = np.concatenate((neg_m1, neg_m2, neg_m3), axis = None)\n",
    "    neg_avg = np.average(neg_arr)\n",
    "\n",
    "    pos_m1 = sentimment_Data[sentimment_Data['model1Sentiment'] == 'positive']['model1Score'].values \n",
    "    pos_m2 = sentimment_Data[sentimment_Data['model2Sentiment'] == 'positive']['model2Score'].values\n",
    "    pos_m3 = sentimment_Data[sentimment_Data['model3Sentiment'] == 'positive']['model3Score'].values\n",
    "    pos_arr = np.concatenate((pos_m1, pos_m2, pos_m3), axis = None)\n",
    "    pos_avg = np.average(pos_arr)\n",
    "\n",
    "    neu_m1 = sentimment_Data[sentimment_Data['model1Sentiment'] == 'neutral']['model1Score'].values\n",
    "    neu_m2 = sentimment_Data[sentimment_Data['model2Sentiment'] == 'neutral']['model2Score'].values  \n",
    "    neu_m3 = sentimment_Data[sentimment_Data['model3Sentiment'] == 'neutral']['model3Score'].values  \n",
    "    neu_arr = np.concatenate((neu_m1, neu_m2, neu_m3), axis = None)\n",
    "\n",
    "    total_vals = len(sentimment_Data) * 3 \n",
    "\n",
    "    positive_per = np.round((len(pos_arr)/total_vals) * 100, 2)\n",
    "    negative_per = np.round((len(neg_arr)/total_vals) * 100, 2)\n",
    "    neutral_per =  np.round((len(neu_arr)/total_vals) * 100, 2)\n",
    "\n",
    "    num_neg, num_pos = len(neg_arr), len(pos_arr)\n",
    "    neg_weight, pos_weight = num_neg / total_vals, num_pos / total_vals\n",
    "    weighted_neg, weighted_pos = neg_avg * neg_weight, pos_avg * pos_weight \n",
    "\n",
    "    try:\n",
    "        if (num_neg / num_pos) > 0.75 and (num_neg / num_pos) < 1.25:\n",
    "            sentimment_ratio = neg_avg / pos_avg\n",
    "        else: \n",
    "            sentimment_ratio = weighted_neg / weighted_pos\n",
    "            \n",
    "    except ZeroDivisionError:\n",
    "        sentimment_ratio = 2\n",
    "    \n",
    "    final_sentimment = 'Negative'\n",
    "    if sentimment_ratio == 1:\n",
    "        final_sentimment = np.random.choice('Negative', 'Positive')\n",
    "    elif sentimment_ratio < 1:\n",
    "        final_sentimment = 'Positive'\n",
    "    \n",
    "    output_dict = {'Negative Percent' : negative_per,\n",
    "                   'Positive Percent' : positive_per,\n",
    "                   'Neutral Percent': neutral_per,\n",
    "                   'Overall Sentiment' : final_sentimment}\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MGY3g442PESc"
   },
   "outputs": [],
   "source": [
    "# social - Reddit, Twitter \n",
    "def Model(data, social): \n",
    "    #get data dic\n",
    "    dataDic = split_by_ticker(data, social)\n",
    "    # get tickers \n",
    "\n",
    "    tickers = list(dataDic.keys())\n",
    "    mentions, negative_per, positive_per, neutral_per, overall = [], [], [], [], []\n",
    "\n",
    "    if social == \"Reddit\":\n",
    "       run_model = run_Reddit \n",
    "    if social == \"Twitter\": \n",
    "       run_model = run_Twitter\n",
    "\n",
    "    for ticker in tickers: \n",
    "        mention_count, stock = run_model(dataDic[ticker]) \n",
    "        results = process_Sentiment(stock)\n",
    "        \n",
    "        mentions.append(mention_count)\n",
    "        negative_per.append(results['Negative Percent'])\n",
    "        positive_per.append(results['Positive Percent'])\n",
    "        neutral_per.append(results['Neutral Percent'])\n",
    "        overall.append(results['Overall Sentiment']) \n",
    "\n",
    "\n",
    "    \n",
    "    output = pd.DataFrame({\"Ticker\": tickers,\n",
    "                          \"Mentions\": mentions, \n",
    "                          \"Negative Percent\": negative_per, \n",
    "                          \"Positive Percent\": positive_per, \n",
    "                          \"Neutral Percent\": neutral_per, \n",
    "                          \"Overall Sentiment\": overall\n",
    "                            })\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "_LqanC8IPH8T",
    "outputId": "c131fc22-6255-4403-90aa-9c591800f85d"
   },
   "outputs": [],
   "source": [
    "#!pip install anvil-uplink\n",
    "import anvil.server\n",
    "from anvil.tables import app_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_tables.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Er8OUPlRPMvc"
   },
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "\n",
    "def add_results(data, date, table): \n",
    "  if table == \"Reddit\": \n",
    "    anvil_db = app_tables.reddit\n",
    "    filt = 0 \n",
    "  if table == \"Twitter\": \n",
    "    anvil_db = app_tables.twitter\n",
    "    filt = 1\n",
    "    \n",
    "\n",
    "  try:\n",
    "    data = data.drop(columns = ['Unnamed: 0'])\n",
    "  except KeyError: \n",
    "    pass\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    #filter out dollar sign for twitter data\n",
    "    ticker = data.loc[i, 'Ticker'][filt:]\n",
    "    mentions = data.loc[i, 'Mentions']\n",
    "    negative_percent = data.loc[i, 'Negative Percent']\n",
    "    positive_percent = data.loc[i, 'Positive Percent']\n",
    "    neutral_percent = data.loc[i, 'Neutral Percent']\n",
    "    overall = data.loc[i, 'Overall Sentiment']\n",
    "\n",
    "\n",
    "    #get correct table\n",
    "    anvil_db.add_row(ticker = ticker, \n",
    "                                      mentions = mentions, \n",
    "                                      negative_percent = negative_percent,\n",
    "                                      positive_percent = positive_percent,\n",
    "                                      neutral_percent = neutral_percent, \n",
    "                                      overall_sentiment = overall, \n",
    "                                      date = date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvzSoF6gWRPX"
   },
   "source": [
    "# Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CGCjwR-CWbqV"
   },
   "outputs": [],
   "source": [
    "def Pull_Data(): \n",
    "  #date format \n",
    "  cur_time = datetime.now().astimezone()\n",
    "  date = cur_time.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "  #apis \n",
    "  apis = [\"Reddit\", \"Twitter\"]\n",
    "\n",
    "  api_dict = {\"Reddit\": pull_reddit, \"Twitter\": pull_twitter}\n",
    "\n",
    "  for api in apis: \n",
    "    print(\"pulling \" + api + \" data...\")\n",
    "    api_func = api_dict[api]\n",
    "    \n",
    "    data = api_func() \n",
    "    print(\"running \" + api + \" model...\")\n",
    "    results = Model(data, api) \n",
    "    \n",
    "    print(\"sending to anvil...\")\n",
    "    done = False\n",
    "    while not done:\n",
    "        try:\n",
    "            anvil.server.connect(\"server_XCAE6PHO23EWSK5QZJ7ULCD6-L3FJQYONABM73HLZ\")\n",
    "            add_results(results, date, api)\n",
    "            done = True\n",
    "            print(api + \" done\")\n",
    "        except Exeption: \n",
    "            pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call: 1\n",
      "pulling Twitter data...\n",
      "$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 457\n",
      " # tweets (filtered): 368 \n",
      "\n",
      "$TSLA -$GME -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 3643\n",
      " # tweets (filtered): 2885 \n",
      "\n",
      "$TWTR -$GME -$TSLA -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 1598\n",
      " # tweets (filtered): 1385 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AMC -$GME -$TSLA -$TWTR -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 1622\n",
      " # tweets (filtered): 1161 \n",
      "\n",
      "$SPY -$GME -$TSLA -$TWTR -$AMC -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 3803\n",
      " # tweets (filtered): 2855 \n",
      "\n",
      "$HMHC -$GME -$TSLA -$TWTR -$AMC -$SPY -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 2\n",
      " # tweets (filtered): 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$DWAC -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 635\n",
      " # tweets (filtered): 486 \n",
      "\n",
      "$AMD -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 230\n",
      " # tweets (filtered): 188 \n",
      "\n",
      "$SST -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 118\n",
      " # tweets (filtered): 56 \n",
      "\n",
      "$AAPL -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 861\n",
      " # tweets (filtered): 677 \n",
      "\n",
      "$AMZN -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 485\n",
      " # tweets (filtered): 318 \n",
      "\n",
      "$NVDA -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 248\n",
      " # tweets (filtered): 202 \n",
      "\n",
      "$TLRY -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 105\n",
      " # tweets (filtered): 50 \n",
      "\n",
      "$NFLX -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 934\n",
      " # tweets (filtered): 713 \n",
      "\n",
      "$QQQ -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 1014\n",
      " # tweets (filtered): 759 \n",
      "\n",
      "$PLTR -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 297\n",
      " # tweets (filtered): 231 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$FB -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 5000\n",
      " # tweets (filtered): 4056 \n",
      "\n",
      "$BABA -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 263\n",
      " # tweets (filtered): 140 \n",
      "\n",
      "$VIX -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$SOFI -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 332\n",
      " # tweets (filtered): 230 \n",
      "\n",
      "$SOFI -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$TEAM -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 263\n",
      " # tweets (filtered): 192 \n",
      "\n",
      "$TEAM -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$RBLX -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 30\n",
      " # tweets (filtered): 15 \n",
      "\n",
      "$RBLX -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RSX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 143\n",
      " # tweets (filtered): 95 \n",
      "\n",
      "$RSX -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$WISH -$OSU -filter:retweets \n",
      " # tweets: 12\n",
      " # tweets (filtered): 12 \n",
      "\n",
      "$WISH -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$OSU -filter:retweets \n",
      " # tweets: 95\n",
      " # tweets (filtered): 51 \n",
      "\n",
      "$OSU -$GME -$TSLA -$TWTR -$AMC -$SPY -$HMHC -$DWAC -$AMD -$SST -$AAPL -$AMZN -$NVDA -$TLRY -$NFLX -$QQQ -$PLTR -$FB -$BABA -$VIX -$SOFI -$TEAM -$RBLX -$RSX -$WISH -filter:retweets \n",
      " # tweets: 0\n",
      " # tweets (filtered): 0 \n",
      "\n",
      "running Twitter model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennis/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/Users/dennis/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending to anvil...\n",
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Development\" as SERVER\n",
      "Twitter done\n",
      "pulling Reddit data...\n",
      "running Reddit model...\n",
      "sending to anvil...\n",
      "Reddit done\n",
      "wait 24 hrs for next pull\n",
      "Anvil websocket closed (code 1006, reason=Going away)\n",
      "Reconnecting Anvil Uplink...\n",
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Development\" as SERVER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dennis/opt/anaconda3/lib/python3.9/site-packages/anvil/server.py\", line 401, in call\n",
      "    return _do_call(args, kwargs, fn_name=fn_name)\n",
      "  File \"/Users/dennis/opt/anaconda3/lib/python3.9/site-packages/anvil/server.py\", line 393, in _do_call\n",
      "    return _threaded_server.do_call(args, kwargs, fn_name=fn_name, live_object=live_object)\n",
      "  File \"/Users/dennis/opt/anaconda3/lib/python3.9/site-packages/anvil/_threaded_server.py\", line 429, in do_call\n",
      "    raise error_from_server\n",
      "anvil._server.AnvilWrappedError: 'Connection to Anvil Uplink server lost'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dennis/opt/anaconda3/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/dennis/opt/anaconda3/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dennis/opt/anaconda3/lib/python3.9/site-packages/anvil/server.py\", line 204, in heartbeat_until_reopened\n",
      "    call(\"anvil.private.echo\", \"keep-alive\")\n",
      "  File \"/Users/dennis/opt/anaconda3/lib/python3.9/site-packages/anvil/server.py\", line 404, in call\n",
      "    raise _server._deserialise_exception(e.error_obj)\n",
      "anvil._server.AnvilWrappedError: 'Connection to Anvil Uplink server lost'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anvil websocket closed (code 1006, reason=Going away)\n",
      "Reconnecting Anvil Uplink...\n",
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Development\" as SERVER\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2000):\n",
    "    print(\"call: \" + str(i))\n",
    "    Pull_Data() \n",
    "    print(\"wait 24 hrs for next pull\")\n",
    "    time.sleep(86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Beta.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
